# -*- coding: utf-8 -*-
"""25-11-21-ADA-PYSPARK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11BhJvBl_2-6stnsuRNHfl-uS4EayV9Ar
"""

from google.colab import drive
drive.mount("/content/gdrive")

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import io

# print working directory
!pwd
# List files and folders
!ls
# Check the open jdk version on colab
!ls /usr/lib/jvm/
# Download and install Java 8
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
# Check if we have java 8 or not
!ls /usr/lib/jvm/
# Download Apache Spark binary: This link can change based on the version. Update this link with the latest version before using
!wget -q https://downloads.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop2.7.tgz
# Unzip file
!tar -xvzf spark-3.2.0-bin-hadoop2.7.tgz
# Install findspark: Adds Pyspark to sys.path at runtime
!pip install -q findspark
# Install pyspark
!pip install pyspark
# Add environmental variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.0-bin-hadoop2.7"

# findspark will locate spark in the system
import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder \
        .master("local") \
        .appName("Hands-on PySpark on Google Colab") \
        .getOrCreate()

filepath="/content/gdrive/MyDrive/ADA /ALL/POI.csv"

POI=spark.read.format('csv').options(header='True',inferSchema='true').load(filepath)

POI.show()

POI.count()

poi_df=POI.toPandas()

poi_df.head()

poi_df.loc[:,"level_id"].str[0:9]

poi_df['level_id_new'] = poi_df.loc[:,"level_id"].str[0:9]

poi_df.head()

poi_df.info()

poi= spark.createDataFrame(poi_df)

poi.printSchema()

filepath="/content/gdrive/MyDrive/ADA /ALL/Tax.csv"

tax=spark.read.format('csv').options(header='True',inferSchema='true').load(filepath)

tax.show()

tax_df=tax.toPandas()

tax_df.head()

tax_df['level_id_new'] =tax_df.loc[:,"level_id"].str[0:9]

tax_df.head()

tax_df.to_csv("tax_new.csv")

filepath="/content/tax_new.csv"

tax=spark.read.format('csv').options(header='True',inferSchema='true').load(filepath)

tax.show()

tax.printSchema()

POI_TAX=poi.join(tax,"level_id_new","left")

print((POI_TAX.count(), len(POI_TAX.columns)))

print((poi.count(), len(POI_TAX.columns)))

print((tax.count(), len(POI_TAX.columns)))

POI_TAX.printSchema()

POI_TAX.groupby("level_id_new").count().show()

POI_TAX=POI_TAX.toPandas()

health_poi_tax=POI_TAX.loc[POI_TAX['level_id_new'] == 'HLT_01_01']

health_poi_tax.head()

health_poi_tax.shape

health_poi_tax.info()

hospital_clinic.info()

hospital_clinic['geohash'].isin(health_poi_tax['geohash']).value_counts()

df_not_common = hospital_clinic.loc[~hospital_clinic['geohash'].isin(health_poi_tax['geohash'])]

df_not_common.shape

hospital_clinic['geohash'].isin(health_poi_tax['geohash']).value_counts()

df_common = hospital_clinic.loc[hospital_clinic['geohash'].isin(health_poi_tax['geohash'])]

df_not_common.to_csv("not.csv")